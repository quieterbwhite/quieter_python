# [Spark函数详解系列之RDD基本转换](https://www.cnblogs.com/MOBIN/p/5373256.html)

**摘要：**

  **RDD：弹性分布式数据集，是一种特殊集合 ‚ 支持多种来源 ‚ 有容错机制 ‚ 可以被缓存 ‚ 支持并行操作，一个RDD代表一个分区里的数据集**

  **RDD有两种操作算子：**

​         **Transformation（转换）：Transformation属于延迟计算，当一个RDD转换成另一个RDD时并没有立即进行转换，仅仅是记住了数据集的逻辑操作**

​         **Ation（执行）：触发Spark作业的运行，真正触发转换算子的计算**

 

**本系列主要讲解Spark中常用的函数操作：**

​         **1.RDD基本转换**

​         **2.键-值RDD转换**

​         **3.Action操作篇**

**本节所讲函数**

**1.map(func)**

**2.flatMap(func)**

**3.mapPartitions(func)**

**4.mapPartitionsWithIndex(func)**

**5.simple(withReplacement,fraction,seed)**

**6.union(ortherDataset)**

**7.intersection(otherDataset)**

**8.distinct([numTasks])**

**9.cartesian(otherDataset)**

**10.coalesce(numPartitions，shuffle)**

**11.repartition(numPartition)**

**12.glom()**

**13.randomSplit(weight:Array[Double],seed)**

 

**基础转换操作：**

 

**1.map(func)：**数据集中的每个元素经过用户自定义的函数转换形成一个新的RDD，新的RDD叫MappedRDD

（例1）

输出：

```
2 4 6 8 10 12 14 16 18 20
```

![img](file:///D:/Program%20Files/WizNote/temp/3ceba066-e690-490c-852b-2dcb2406bd38_128_files/72e8aac9-8635-41a9-886d-915978b96ac8.png)

**(RDD依赖图：红色块表示一个RDD区，黑色块表示该分区集合，下同)**

![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410013320031-1234218566.png)

 

**2.flatMap(func):**与map类似，但每个元素输入项都可以被映射到0个或多个的输出项，最终将结果”扁平化“后输出

（例2）

输出：

```
1 1 2 1 2 3 1 2 3 4 1 2 3 4 5
```

如果是map函数其输出如下：

```
Range(1) Range(1, 2) Range(1, 2, 3) Range(1, 2, 3, 4) Range(1, 2, 3, 4, 5)
```

 **(RDD依赖图)**

![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410014620703-1505030822.png)

![img](file:///D:/Program%20Files/WizNote/temp/3ceba066-e690-490c-852b-2dcb2406bd38_128_files/dd025a7a-d21d-414e-826c-aac68ced53aa.png)

 

**3.mapPartitions(func):**类似与map，map作用于每个分区的每个元素，但mapPartitions作用于每个分区工

func的类型：Iterator[T] => Iterator[U]

假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,当在映射的过程中不断的创建对象时就可以使用mapPartitions比map的效率要高很多，比如当向数据库写入数据时，如果使用map就需要为每个元素创建connection对象，但使用mapPartitions的话就需要为每个分区创建connetcion对象

(例3)：输出有女性的名字：

输出：

```
kpop lucy
```

其实这个效果可以用一条语句完成

之所以不那么做是为了演示函数的定义

![img](file:///D:/Program%20Files/WizNote/temp/3ceba066-e690-490c-852b-2dcb2406bd38_128_files/d1f4e186-be31-4d7f-ad32-dc03301ce750.png)

  **(RDD依赖图)**

**![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410014739172-1332748150.png)**

 

**4.mapPartitionsWithIndex(func):**与mapPartitions类似，不同的时函数多了个分区索引的参数

func类型：(Int, Iterator[T]) => Iterator[U]

（例4）：将例3橙色的注释部分去掉即是

输出：（带了分区索引）

```
[0]kpop [1]lucy
```

 

**5.sample(withReplacement,fraction,seed):**以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样

(例5)：从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）

 

**6.union(ortherDataset):**将两个RDD中的数据集进行合并，最终返回两个RDD的并集，若RDD中存在相同的元素也不会去重

输出：

```
1 2 3 3 4 5
```

　　

**7.intersection(otherDataset):**返回两个RDD的交集

输出：

```
3 4
```

 

**8.distinct([numTasks]):**对RDD中的元素进行去重

输出：

```
1 6 9 5 2
```

 

**9.cartesian(otherDataset):**对两个RDD中的所有元素进行笛卡尔积操作

输出：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
(1,2)
(1,3)
(1,4)
(1,5)
(2,2)
(2,3)
(2,4)
(2,5)
(3,2)
(3,3)
(3,4)
(3,5)
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 **(RDD依赖图)**

 ![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410014905562-172016304.png)

![img](file:///D:/Program%20Files/WizNote/temp/3ceba066-e690-490c-852b-2dcb2406bd38_128_files/59507441-d76c-45c2-bcf9-1a366db59e96.png)

 

**10.coalesce(numPartitions，shuffle):**对RDD的分区进行重新分区，shuffle默认值为false,当shuffle=false时，不能增加分区数

目,但不会报错，只是分区个数还是原来的

(例9:）**shuffle=false**

输出：

```
重新分区后的分区个数:3
//分区后的数据集
List(1, 2, 3, 4)
List(5, 6, 7, 8)
List(9, 10, 11, 12, 13, 14, 15, 16) 
```

　

(例9.1:）**shuffle=true**

输出：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
重新分区后的分区个数:5
RDD依赖关系:(5) MapPartitionsRDD[4] at coalesce at Coalesce.scala:14 []
| CoalescedRDD[3] at coalesce at Coalesce.scala:14 []
| ShuffledRDD[2] at coalesce at Coalesce.scala:14 []
+-(4) MapPartitionsRDD[1] at coalesce at Coalesce.scala:14 []
| ParallelCollectionRDD[0] at parallelize at Coalesce.scala:13 []
//分区后的数据集
List(10, 13)
List(1, 5, 11, 14)
List(2, 6, 12, 15)
List(3, 7, 16)
List(4, 8, 9) 
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 **(RDD依赖图:coalesce(3,flase))**

 **![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410015000640-882155772.png)**

 

![img](file:///D:/Program%20Files/WizNote/temp/ba3f8894-8695-4402-9657-92c74bfa2afb.png)

 **(RDD依赖图:coalesce(3,true))**

 ![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410015009703-40958337.png)

 

**11.repartition(numPartition):**是函数coalesce(numPartition,true)的实现，效果和例9.1的coalesce(numPartition,true)的一样

 

 

**12.glom():**将RDD的每个分区中的类型为T的元素转换换数组Array[T]

```
 
```

输出：

```
int[] //说明RDD中的元素被转换成数组Array[Int]
```

 ![img](https://images2015.cnblogs.com/blog/776259/201604/776259-20160410015053218-1975193443.png)

![img](file:///D:/Program%20Files/WizNote/temp/8420ca34-b877-4bc1-972b-7bcd15b5bb78.png)

 

**13.randomSplit(weight:Array[Double],seed):**根据weight权重值将一个RDD划分成多个RDD,权重越高划分得到的元素较多的几率就越大

输出：

```
2 4
3 8 9
1 5 6 7 10
```

 

 以上例子源码地址：<https://github.com/Mobin-F/SparkExample/tree/master/src/main/scala/com/mobin/SparkRDDFun/TransFormation/KVRDD>