# 裁判文书网爬虫（2018-05-16更新）

> 最近有一些朋友通过QQ私聊我，一起交流了一些关于验证码、采集策略相关的问题，所以最近花了些时间更新了部分功能模块，但由于最近比较忙，所以没有时间对代码进行重写，比较杂乱，但所有功能模块都实现在相关函数内，打了部分注释，大家可以根据自己的需求重写代码，进行系统封装，本项目旨在分享、交流、学习。

## 新功能模块 ##

> 新功能模块的说明见文末.

 - 获取左侧分类类目
 - 根据DocID获取文书详细信息
 - 根据DocID下载文书doc文档
 - 一级验证码识别

## 项目简介 ##
本项目旨在分享文书网数据采集学习过程的一些想法，很高兴能和大家一起交流、一起学习、一起进步.目前实现了下列功能模块：

 - 获取分类类目及其数据量信息
 - 根据搜索条件采集数据
 - 根据DocID采集相关文书具体信息
 - 根据DocID下载相关doc文档
 - 一级验证码识别

<注>采集文书网仅仅是为了学习，所以并没有进行真正的大量采集，所以二级验证码和代理ip切换等情况在代码中并没有实现.

 
## 环境 ##
代码编写环境为python3.4.3，使用到的相关库在requirement.txt

## 运行(命令行) ##

     1. >git clone https://github.com/sixs/wenshu_spider
     2. >cd wenshu_spider 
     3. >virtualenv --no-site-packages venv # 创建python虚拟环境
        或>virtualenv --python=python3 --no-site-packages # 指定python版本
     4. >cd venv/Scripts & activate # 进入python虚拟环境
     5. (venv)>cd ../../2018-05-16 & pip install -r requirement.txt # 安装相关库
     6. (venv)>python court.py

## 小谈采集策略 ##

> 想必小伙伴们都知道文书有一个反爬就是，每次查询最多只能查看**2000条**数据，所以要做到全量采集不遗漏需要一定的策略，下面是对于采集策略本人的一些看法，如果有不足或错误的地方，大家不要见怪哈~

> 基本思想：添加检索条件，对数据量进行切割操作，每次检索量切割到2000以下，再请求采集数据即可获取全量数据.

**具体实现：**

1、首先先把搜索条件设为最大范围，即Param字段中的条件设置如下：

    Param:全文检索:*
搜索出来45467118左右个结果，这是文书网数据的总量。
2、由于此时搜索的数据量很大，要想切割成较小的数据量，则需要切割成很多份，所以首选的条件是加入裁判日期这个条件，对于2015-2018年这几年，日数据量平均在1w左右，所以应当精确到按天采集.即Param如下：

    Param:全文检索:*,裁判日期:2018-05-15 TO 2018-05-16
3、加入时间条件后，数据量比较接近2000了，但是部分天数数据量仍然比较大，所以应当加入其他条件进行切割。为了切割理想化（每次切割尽量接近2000），则可以分为以下三种情况：

 - 数据量仍然较大，数据量 > 1w，则选用法院地域进行切割比较好，因为一般有十几个省份法院地域可以进行切割.即Param如下：
 - 数据量不算太大，但仍然大于2000，即2000 - 10000，此时法院层级、案件类型这些条件都是比较好的选择，达到切割目的又不至于切割过细增加采集成本.
 - 数据量小于2000，则可以直接加入采集队列.

4、如果3中的条件切割后数据量仍然超过2000(一般此时数据量不会太大，接近2000)，从法院层级、案件类型、案由、审判程序等中选择筛选条件相对合适.


**小 Tips**

1、分割过程应当根据实际的数据量选择分割条件，为了节省采集过程的资源投入，最理想的情况是加入条件后，每份的数据量应当尽可能接近2000，这样采集的次数相对较少.
2、由于文书网数据量较大，全量采集过程花费的精力、时间、资源较大，所以比较好的一个策略是先进行条件切割，对满足采集条件(搜索数据量 <= 2000)先进行存储，而后统一取出构建采集请求，采用分布式等策略进行统一调度分配后进行采集、入库.

**新功能模块说明**

> 新更新的功能模块在代码中封装在了对应的函数中，并没有进行调用，如有需要可以在代码进行调用.

新功能模块更新原因如下：

 - **一级验证码识别**：全量采集过程仅靠切换代理ip应对验证码反爬，成本超高，破解验证码可以减少代理方面的成本，一级验证码比较简单，而二级验证码相对比较复杂一些，所以就没有去研究了.
 - **获取左侧分类类**目:对数据量进行条件切割时，部分条件并不是所有取值情况都满足(有数据),例如某一天中并不是所有省份都有数据，所以可以先获取左侧分类类目，获取条件的所有可取值及该条件下的数据量，过滤掉无用的条件取值，可以节省采集工作量，亦可为下一步切割策略提供参考.
 - **根据DocID获取文书详细信息**：由搜索出来的文书数据字段中的content字段是不完整的，而且缺少最后更新的时间等信息，所以可以在全量采集完成的基础上，从数据库中取出DocID字段进行二次采集，填补缺失字段.
 - **根据DocID下载文书doc文档**：文书官网中提供文档下载功能，于是添加了模拟请求下载doc文档功能模块.


最后，真心希望这个项目可以帮到小伙伴们，如果你觉得不错或者对你有帮助，给个star呗，也算是对我学习路上的一种鼓励！哈哈哈~

                                                                                            
